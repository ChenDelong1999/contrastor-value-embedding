train_args:
  num_epochs: 3 
  learning_rate: 2.0e-4
  weight_decay: 0.01
  warmup_steps: 700
  cooldown_steps: null
  checkpoint: null
  wandb: false
  wandb_project_name: "text-contrastors"
  wandb_entity: "gpt4all"
  log_grads_every: 100
  log_lr_every: 10
  save_every: 2500 
  chunk_size: 64
  output_dir: "ckpts/nomic-embed-text-v1-len2048-bs64-4gpu-[1,20]-QA-per-view"
  # if using deepspeed, this will be ignored
  schedule_type: "inverse_sqrt"
  max_grad_norm: 1.0
  adam_beta1: 0.9
  adam_beta2: 0.999
  grad_cache: true
  loss_fn: "clip"
  use_fp8: false
  clamp_logits: false
  logit_max: 100

model_args:
  logit_scale: 50
  trainable_logit_scale: false
  model_type: "encoder"
  seq_len: 2048
  rotary_emb_fraction: 0.0
  pad_vocab_to_multiple_of: 64 
  use_rms_norm: false
  activation_function: "gelu"
  pooling: "mean"
  encoder: true
  add_prefix: true
  tokenizer_name: "bert-base-uncased"
  model_name: "nomic-ai/nomic-embed-text-v1"
  qkv_proj_bias: false
  mlp_fc1_bias: false
  mlp_fc2_bias: false

contrastive_data_args:
  value_embedding: True
  streaming: False
  dataset_path: '/share/value-embedding/datasets/20240314_gen_qa_pairs_translated_to_english/train'
  workers: 4 
  batch_size: 64
  min_qa_per_view: 1
  max_qa_per_view: 20
  seed: 42
