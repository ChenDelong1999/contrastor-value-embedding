{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MultiViewValueQADataset(Dataset):\n",
    "    def __init__(self, dataset_path, qa_per_view=1):\n",
    "        \n",
    "        self.all_qa_samples = self.read_samples(dataset_path)\n",
    "        self.value_idx_map = {i: k for i,k in enumerate(self.all_qa_samples.keys())}\n",
    "        self.qa_per_view = qa_per_view\n",
    "        \n",
    "        num_pair = []\n",
    "        for k, v in self.all_qa_samples.items():\n",
    "            num_pair.append(len(v))\n",
    "            # print(f\" [Value Dataset] {len(v)} samples in {k}\")\n",
    "        self.end_value_idx = np.array(num_pair).cumsum()\n",
    "\n",
    "    def read_samples(self, path):\n",
    "        dfs = []\n",
    "        for path in sorted(glob.glob(f'{path}/*.csv*')):\n",
    "            # print(f\" [Value Dataset] Reading {path}\")\n",
    "            dfs.append(pd.read_csv(path))\n",
    "        df = pd.concat(dfs)\n",
    "        df = df.dropna(subset=['answer']).reset_index(drop=True)\n",
    "        df['qa'] = df.apply(lambda x: f\"Q: {x['question'].strip()} A: {x['answer'].strip()}\", axis='columns')\n",
    "\n",
    "        samples = {f'{k[0]}${k[1]}': tdf['qa'].tolist() for k, tdf in df.groupby(['model', 'lang'])}\n",
    "        # print(f\" [Value Dataset] Total {len(samples)} values loaded\")\n",
    "        return samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.end_value_idx[-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        value_idx = np.argmax(self.end_value_idx > idx)       \n",
    "        value_key = self.value_idx_map[value_idx]\n",
    "        qa_data = self.all_qa_samples[value_key]\n",
    "\n",
    "        view_1 = '\\n'.join(random.sample(qa_data, min(len(qa_data), self.qa_per_view)))\n",
    "        view_2 = '\\n'.join(random.sample(qa_data, min(len(qa_data), self.qa_per_view)))\n",
    " \n",
    "        sample = {'query': view_1, 'document': view_2, 'dataset_name': value_key} \n",
    "        return sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import torch\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "for n_samples, dataset_path in [\n",
    "    (10000, '/share/value-embedding/datasets/20240314_gen_qa_pairs_translated_to_english/train'),\n",
    "    (5000, '/share/value-embedding/datasets/20240314_gen_qa_pairs_translated_to_english/oos'),\n",
    "]:\n",
    "    for qa_per_view, title, model_path in  [\n",
    "            # (1, \"single QA per view all-mpnet-base-v2 (Mar29)\", \"/share/value-embedding/checkpoints/Mar29_19-24-52_eez116_[translated]-[all-mpnet-base-v2]-[infonce]/140340\"),\n",
    "            # (5, \"5 QA per view reward-model-deberta-v3-base (Apr07)\", \"/share/value-embedding/checkpoints/Apr07_17-33-46_eez116_arxiv-v1-1ep-[reward-model-deberta-v3-base]-[infonce-5_qa_per_view]/18712\"),\n",
    "\n",
    "            # (1, \"single QA per view nomic (context=128 tokens)\", \"/home/dchenbs/workspace/contrastors/src/contrastors/ckpts/nomic-embed-text-v1-len128-bs128-4gpu/sbert\"),\n",
    "            # (5, \"5 QA per view nomic (context=512 tokens)\", \"/home/dchenbs/workspace/contrastors/src/contrastors/ckpts/nomic-embed-text-v1-len512-bs128-4gpu/sbert\"),\n",
    "\n",
    "            (100, \"nomic (context=2048 tokens)\",  \"/home/dchenbs/workspace/contrastors/src/contrastors/ckpts/nomic-embed-text-v1-len2048-bs128-4gpu/sbert\"),\n",
    "            (100, \"nomic (context=4096 tokens)\",\"/home/dchenbs/workspace/contrastors/src/contrastors/ckpts/nomic-embed-text-v1-len4096-bs128-4gpu/sbert\")\n",
    "        ]:\n",
    "\n",
    "        print(f\"Using {qa_per_view} QA per view model {model_path}\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        dataset = MultiViewValueQADataset(dataset_path, qa_per_view=qa_per_view)\n",
    "        model = SentenceTransformer(model_path, trust_remote_code=True).to('cuda')\n",
    "\n",
    "        # gather samples\n",
    "        samples = {}\n",
    "        embeddings = {}\n",
    "        for value in dataset.value_idx_map.values():\n",
    "            samples[value] = []\n",
    "            embeddings[value] = []\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            sample = dataset[random.randint(0, len(dataset)-1)]\n",
    "            samples[sample['dataset_name']].append(sample['query'])\n",
    "\n",
    "        # encoding\n",
    "        for value in dataset.value_idx_map.values():\n",
    "            print(f\"Extract embeddings for {len(samples[value])} samples in {value}\")\n",
    "            embeddings[value] = model.encode(samples[value], convert_to_tensor=True, show_progress_bar=True, batch_size=8)\n",
    "\n",
    "        # tsne\n",
    "        tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "        X = np.concatenate([v.cpu().numpy() for v in embeddings.values()])\n",
    "        X_embedded = tsne.fit_transform(X)\n",
    "\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        sns.scatterplot(x=X_embedded[:,0], y=X_embedded[:,1], hue=np.concatenate([[k]*v.shape[0] for k,v in embeddings.items()]), palette='tab20', size=0.5, alpha=0.5)\n",
    "        plt.title(title+'\\n'+model_path)\n",
    "\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "value_embedding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
